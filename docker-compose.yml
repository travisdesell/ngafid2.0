version: "1.0"

# Common configuration for all NGAFID services
# No environment variables needed - everything configured via properties files
x-ngafid-service-common-base: &ngafid-service-common-base
  extra_hosts:
    - "host.docker.internal:host-gateway"

x-ngafid-service-common: &ngafid-service-common
  <<: *ngafid-service-common-base
  depends_on:
    ngafid-kafka-topics:
      condition: service_completed_successfully
  volumes:
    # Mount the properties file and data directories
    - ./ngafid-core/src/main/resources/ngafid.properties:/app/ngafid.properties:ro
    - ./data/uploads:/mnt/uploads
    - ./data/archive:/mnt/archive
    - ./ngafid-static:/mnt/static
    - ./data/terrain:/mnt/terrain

services:
  base:
    build:
      context: .
      # No build args needed - everything configured via properties files
    image: ngafid-base

  # This must run and complete before everything else to create the Kafka topics
  ngafid-kafka-topics:
    build:
      context: .
      dockerfile: ngafid-core/Dockerfile.topics
    <<: *ngafid-service-common-base
    depends_on:
      kafka:
        condition: service_started

  # Reads emails from email kafka topic and sends them using the supplied credentials
  ngafid-email-consumer:
    build:
      context: .
      dockerfile: ngafid-core/Dockerfile.email
    <<: *ngafid-service-common

  # AirSync sync daemon
  # ngafid-airsync-importer:
  #   build:
  #     context: .
  #     dockerfile: ngafid-airsync/Dockerfile
  #   <<: *ngafid-service-common

  # Upload processor. Upload topic is created with 6 partitions by default, so up to 6 replicas would work.
  ngafid-upload-consumer:
    build:
      context: .
      dockerfile: ngafid-data-processor/Dockerfile
    deploy:
      mode: replicated
      replicas: 2
    <<: *ngafid-service-common

  # Webserver
  ngafid-www:
    build:
      context: .
      dockerfile: ngafid-www/Dockerfile
    ports:
      - "8181:8181"
    <<: *ngafid-service-common

  # Event Consumer: reads from event topic and computes the events
  ngafid-event-consumer:
    build:
      context: .
      dockerfile: ngafid-data-processor/Dockerfile.event-consumer
    <<: *ngafid-service-common

  # Event Observer: scans database for events that have not been computed, and computes them.
  # This can also be used to re-compute events as potential duplicates are deleted.
  ngafid-event-observer:
    build:
      context: .
      dockerfile: ngafid-data-processor/Dockerfile.event-observer
    <<: *ngafid-service-common


  kafka:
    image: apache/kafka-native
    ports:
      - "9092:9092"
    environment:
      # Configure listeners for both docker and host communication
      KAFKA_LISTENERS: CONTROLLER://localhost:9091,HOST://0.0.0.0:9092,DOCKER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: HOST://localhost:9092,DOCKER://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,DOCKER:PLAINTEXT,HOST:PLAINTEXT

      # Settings required for KRaft mode
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@localhost:9091

      # Listener to use for broker-to-broker communication
      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER

      # Required for a single node cluster
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
